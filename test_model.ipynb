{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflite-runtime in /home/andrews/.local/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/andrews/.local/lib/python3.8/site-packages (from tflite-runtime) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tflite-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 22:49:02.306241: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 22:49:02.461994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 22:49:02.464958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 22:49:03.437732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_nurse = \"./Test/NURSE.mp4\"\n",
    "video_path_taste_good = \"./Test/TASTE GOOD.mp4\"\n",
    "video_path_cloudy = \"./Test/CLOUDS, cloudy, cloud.mp4\"\n",
    "video_path_swim = \"./Test/swim.mp4\"\n",
    "video_path_tmp = \"./Test/tmp5ts_s_z0.mp4\"\n",
    "\n",
    "video_path_up = \"./Test/up.mp4\"\n",
    "cap = cv2.VideoCapture(video_path_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "QApplication: invalid style override 'adwaita' passed, ignoring it.\n",
      "\tAvailable styles: Windows, Fusion\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "data_list = []\n",
    "ROWS_PER_FRAME = 543  # Constant number of landmarks per frame\n",
    "\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1) as face_mesh, \\\n",
    "     mp_hands.Hands(static_image_mode=False, max_num_hands=2) as hands, \\\n",
    "     mp_pose.Pose(static_image_mode=False) as pose:\n",
    "\n",
    "    frame_number = 0\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the BGR image to RGB for Mediapipe\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process face landmarks\n",
    "        results_face = face_mesh.process(image_rgb)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "                data_list.append([frame_number, f\"{frame_number}-face-{idx}\", \"face\", idx, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # Process hand landmarks\n",
    "        results_hands = hands.process(image_rgb)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "                for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                    data_list.append([frame_number, f\"{frame_number}-right_hand-{idx}\", \"right-hand\", idx, landmark.x, landmark.y, landmark.z])\n",
    "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Process pose landmarks\n",
    "        results_pose = pose.process(image_rgb)\n",
    "        if results_pose.pose_landmarks:\n",
    "            pose_landmarks = results_pose.pose_landmarks.landmark\n",
    "            for idx, landmark in enumerate(pose_landmarks):\n",
    "                data_list.append([frame_number, f\"{frame_number}-pose-{idx}\", \"pose\", idx, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # Pad the landmarks with NaN values if the number of landmarks is less than ROWS_PER_FRAME\n",
    "        while len(data_list) < (frame_number + 1) * ROWS_PER_FRAME:\n",
    "            data_list.append([frame_number, f\"{frame_number}-right_hand-{len(data_list) % ROWS_PER_FRAME}\", \"right-hand\", len(data_list) % ROWS_PER_FRAME, np.nan, np.nan, np.nan])\n",
    "\n",
    "        # Draw the landmarks on the frame (optional)\n",
    "        mp_drawing.draw_landmarks(image, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "        mp_drawing.draw_landmarks(image, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('MediaPipe', image)\n",
    "        frame_number += 1\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list, columns=[\"frame\", \"row_id\", \"type\", \"landmark_index\", \"x\", \"y\", \"z\"])\n",
    "df.to_parquet(\"extracted_features.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('./1006440534.parquet')\n",
    "test_data_kaggle = pd.read_parquet('1001373962.parquet')\n",
    "test_data_kaggle2 = pd.read_parquet('./100015657.parquet')\n",
    "test_data_kaggle3 = pd.read_parquet('./1003700302.parquet')\n",
    "test_data_kaggle4 = pd.read_parquet('./1007127288.parquet')\n",
    "test_data_my_own = pd.read_parquet('extracted_features.parquet')\n",
    "test_data_my_own['frame'] = test_data_my_own['frame'].astype('int16')\n",
    "test_data_my_own['landmark_index'] = test_data_my_own['landmark_index'].astype('int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_relevant_data_subset(pq_path, ROWS_PER_FRAME = 543):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns) \n",
    "    n_frames = int( len(data) / ROWS_PER_FRAME)\n",
    "    print(f\"Data: {len(data)} Number of Frames: {n_frames}\")\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# demo_raw_data = load_relevant_data_subset('./1006440534.parquet')\n",
    "demo_raw_data = load_relevant_data_subset('./1001373962.parquet')\n",
    "# demo_raw_data = load_relevant_data_subset('./1003700302.parquet', test_data_kaggle3['frame'].nunique())\n",
    "# demo_raw_data = load_relevant_data_subset('./extracted_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 3258 Number of Frames: 6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORD2SIGN = {206: 'sticky',\n",
    " 20: 'before',\n",
    " 178: 'pretty',\n",
    " 114: 'hen',\n",
    " 221: 'tomorrow',\n",
    " 230: 'up',\n",
    " 25: 'blow',\n",
    " 236: 'weus',\n",
    " 184: 'read',\n",
    " 191: 'say',\n",
    " 248: 'zebra',\n",
    " 189: 'sad',\n",
    " 62: 'drawer',\n",
    " 5: 'animal',\n",
    " 167: 'pen',\n",
    " 60: 'donkey',\n",
    " 41: 'cheek',\n",
    " 51: 'cowboy',\n",
    " 192: 'scissors',\n",
    " 181: 'quiet',\n",
    " 63: 'drink',\n",
    " 94: 'girl',\n",
    " 200: 'sleepy',\n",
    " 249: 'zipper',\n",
    " 171: 'pig',\n",
    " 13: 'bad',\n",
    " 9: 'arm',\n",
    " 61: 'down',\n",
    " 123: 'if',\n",
    " 240: 'why',\n",
    " 166: 'pajamas',\n",
    " 203: 'snow',\n",
    " 137: 'loud',\n",
    " 195: 'shirt',\n",
    " 31: 'brown',\n",
    " 146: 'moon',\n",
    " 23: 'bird',\n",
    " 210: 'sun',\n",
    " 76: 'fast',\n",
    " 1: 'after',\n",
    " 54: 'cute',\n",
    " 77: 'feet',\n",
    " 4: 'alligator',\n",
    " 87: 'food',\n",
    " 113: 'hello',\n",
    " 93: 'giraffe',\n",
    " 180: 'puzzle',\n",
    " 211: 'table',\n",
    " 132: 'like',\n",
    " 153: 'no',\n",
    " 122: 'icecream',\n",
    " 67: 'duck',\n",
    " 69: 'elephant',\n",
    " 141: 'many',\n",
    " 18: 'bedroom',\n",
    " 205: 'stay',\n",
    " 74: 'fall',\n",
    " 246: 'yourself',\n",
    " 183: 'rain',\n",
    " 135: 'listen',\n",
    " 44: 'chocolate',\n",
    " 124: 'into',\n",
    " 11: 'awake',\n",
    " 40: 'chair',\n",
    " 7: 'any',\n",
    " 155: 'nose',\n",
    " 118: 'home',\n",
    " 161: 'open',\n",
    " 58: 'dog',\n",
    " 50: 'cow',\n",
    " 241: 'will',\n",
    " 149: 'mouth',\n",
    " 177: 'pretend',\n",
    " 172: 'pizza',\n",
    " 75: 'farm',\n",
    " 163: 'outside',\n",
    " 234: 'water',\n",
    " 81: 'finish',\n",
    " 159: 'old',\n",
    " 121: 'hungry',\n",
    " 112: 'helicopter',\n",
    " 130: 'lamp',\n",
    " 222: 'tongue',\n",
    " 194: 'shhh',\n",
    " 6: 'another',\n",
    " 103: 'gum',\n",
    " 214: 'thankyou',\n",
    " 128: 'kiss',\n",
    " 101: 'grass',\n",
    " 64: 'drop',\n",
    " 157: 'now',\n",
    " 233: 'wake',\n",
    " 116: 'hide',\n",
    " 201: 'smile',\n",
    " 226: 'toy',\n",
    " 216: 'there',\n",
    " 147: 'morning',\n",
    " 10: 'aunt',\n",
    " 102: 'green',\n",
    " 36: 'car',\n",
    " 213: 'taste',\n",
    " 39: 'cereal',\n",
    " 207: 'store',\n",
    " 66: 'dryer',\n",
    " 162: 'orange',\n",
    " 218: 'thirsty',\n",
    " 83: 'first',\n",
    " 45: 'clean',\n",
    " 3: 'all',\n",
    " 198: 'sick',\n",
    " 129: 'kitty',\n",
    " 96: 'glasswindow',\n",
    " 202: 'snack',\n",
    " 150: 'nap',\n",
    " 53: 'cut',\n",
    " 73: 'face',\n",
    " 99: 'grandma',\n",
    " 209: 'stuck',\n",
    " 91: 'garbage',\n",
    " 115: 'hesheit',\n",
    " 95: 'give',\n",
    " 104: 'hair',\n",
    " 125: 'jacket',\n",
    " 165: 'owl',\n",
    " 82: 'fireman',\n",
    " 227: 'tree',\n",
    " 16: 'because',\n",
    " 17: 'bed',\n",
    " 30: 'brother',\n",
    " 143: 'minemy',\n",
    " 127: 'jump',\n",
    " 245: 'yesterday',\n",
    " 145: 'mom',\n",
    " 111: 'hear',\n",
    " 174: 'police',\n",
    " 223: 'tooth',\n",
    " 212: 'talk',\n",
    " 224: 'toothbrush',\n",
    " 164: 'owie',\n",
    " 47: 'closet',\n",
    " 169: 'penny',\n",
    " 24: 'black',\n",
    " 85: 'flag',\n",
    " 238: 'white',\n",
    " 134: 'lips',\n",
    " 231: 'vacuum',\n",
    " 8: 'apple',\n",
    " 105: 'happy',\n",
    " 151: 'napkin',\n",
    " 92: 'gift',\n",
    " 70: 'empty',\n",
    " 46: 'close',\n",
    " 52: 'cry',\n",
    " 138: 'mad',\n",
    " 49: 'clown',\n",
    " 204: 'stairs',\n",
    " 42: 'child',\n",
    " 173: 'please',\n",
    " 65: 'dry',\n",
    " 72: 'eye',\n",
    " 235: 'wet',\n",
    " 32: 'bug',\n",
    " 109: 'haveto',\n",
    " 228: 'uncle',\n",
    " 199: 'sleep',\n",
    " 176: 'potty',\n",
    " 29: 'boy',\n",
    " 136: 'look',\n",
    " 107: 'hate',\n",
    " 71: 'every',\n",
    " 12: 'backyard',\n",
    " 22: 'better',\n",
    " 84: 'fish',\n",
    " 56: 'dance',\n",
    " 139: 'make',\n",
    " 98: 'goose',\n",
    " 38: 'cat',\n",
    " 232: 'wait',\n",
    " 14: 'balloon',\n",
    " 247: 'yucky',\n",
    " 2: 'airplane',\n",
    " 88: 'for',\n",
    " 126: 'jeans',\n",
    " 154: 'noisy',\n",
    " 142: 'milk',\n",
    " 239: 'who',\n",
    " 90: 'frog',\n",
    " 35: 'can',\n",
    " 215: 'that',\n",
    " 117: 'high',\n",
    " 244: 'yes',\n",
    " 196: 'shoe',\n",
    " 108: 'have',\n",
    " 48: 'cloud',\n",
    " 170: 'person',\n",
    " 187: 'ride',\n",
    " 34: 'callonphone',\n",
    " 37: 'carrot',\n",
    " 100: 'grandpa',\n",
    " 120: 'hot',\n",
    " 131: 'later',\n",
    " 229: 'underwear',\n",
    " 0: 'TV',\n",
    " 140: 'man',\n",
    " 217: 'think',\n",
    " 220: 'time',\n",
    " 80: 'finger',\n",
    " 86: 'flower',\n",
    " 15: 'bath',\n",
    " 28: 'book',\n",
    " 193: 'see',\n",
    " 208: 'story',\n",
    " 26: 'blue',\n",
    " 78: 'find',\n",
    " 148: 'mouse',\n",
    " 79: 'fine',\n",
    " 179: 'puppy',\n",
    " 55: 'dad',\n",
    " 21: 'beside',\n",
    " 225: 'touch',\n",
    " 89: 'frenchfries',\n",
    " 188: 'room',\n",
    " 19: 'bee',\n",
    " 27: 'boat',\n",
    " 156: 'not',\n",
    " 59: 'doll',\n",
    " 97: 'go',\n",
    " 190: 'same',\n",
    " 144: 'mitten',\n",
    " 160: 'on',\n",
    " 57: 'dirty',\n",
    " 182: 'radio',\n",
    " 197: 'shower',\n",
    " 186: 'refrigerator',\n",
    " 158: 'nuts',\n",
    " 175: 'pool',\n",
    " 242: 'wolf',\n",
    " 243: 'yellow',\n",
    " 110: 'head',\n",
    " 237: 'where',\n",
    " 33: 'bye',\n",
    " 133: 'lion',\n",
    " 152: 'night',\n",
    " 106: 'hat',\n",
    " 43: 'chin',\n",
    " 68: 'ear',\n",
    " 168: 'pencil',\n",
    " 119: 'horse',\n",
    " 219: 'tiger',\n",
    " 185: 'red'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED :  have [108]\n"
     ]
    }
   ],
   "source": [
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "interpreter = tflite.Interpreter(\"./model.tflite\")\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "prediction_fn(inputs=demo_raw_data)\n",
    "\n",
    "\n",
    "output = prediction_fn(inputs=demo_raw_data)\n",
    "sign = output['outputs'].argmax()\n",
    "print(\"PRED : \", ORD2SIGN.get(sign), f'[{sign}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
